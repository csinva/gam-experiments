{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scipy.io import arff\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import imodels\n",
    "import imodels.util.data_util\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = pd.read_html('https://www.uco.es/kdis/mllresources/#EnronDesc')\n",
    "# # drop last column\n",
    "# df = html[0]\n",
    "# df = df.iloc[:, :-1]\n",
    "# # convert multiindex to single index\n",
    "# df.columns = [col[0] for col in df.columns.values]\n",
    "# df.to_csv('multitask.csv')\n",
    "ovw = pd.read_csv('multitask.csv')\n",
    "vals = ovw.Dataset.str.lower().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# manually download each dataset in mulan format to folder 'dsets'\n",
    "dsets = [d for d in sorted(os.listdir('dsets'))\n",
    "         if os.path.isdir(join('dsets', d))]\n",
    "os.makedirs('processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arff_to_df(arff_file, xml_file):\n",
    "    # convert to csv\n",
    "    data, meta = arff.loadarff(arff_file)\n",
    "\n",
    "    with open(xml_file, 'r') as file:\n",
    "        # Parse the XML file into a dictionary\n",
    "        targets = xmltodict.parse(file.read())\n",
    "    targets = [d['@name'] for d in [targets['labels']['label']][0]]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    for target in targets:\n",
    "        assert target in df.columns\n",
    "\n",
    "    # append __target to each target column\n",
    "    df.columns = [\n",
    "        f'{col}__target' if col in targets else col for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "for dset in tqdm(dsets):\n",
    "    files = os.listdir(join('dsets', dset))\n",
    "    arff_file = [f for f in files if f.endswith('.arff')][0]\n",
    "    xml_file = [f for f in files if f.endswith('.xml')][0]\n",
    "\n",
    "    try:\n",
    "        dset_name = dset.replace(\"_Mulan\", '')\n",
    "        dset_name = dset_name.lower()\n",
    "        if dset_name.replace('_', '-') in vals:\n",
    "            dset_name = dset_name.replace('_', '-')\n",
    "        elif dset_name.replace('-', '_') in vals:\n",
    "            dset_name = dset_name.replace('-', '_')\n",
    "        out_file = join('processed', f'{dset_name}.csv')\n",
    "        if not os.path.exists(out_file):\n",
    "            df = arff_to_df(join('dsets', dset, arff_file),\n",
    "                            join('dsets', dset, xml_file))\n",
    "            df.to_csv(join('processed', f'{dset_name}.csv'), index=False)\n",
    "    except:\n",
    "        print(f'Failed to process {dset}')\n",
    "        # print error trace\n",
    "        # traceback.print_exc()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process byte strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = sorted(\n",
    "    [d for d in os.listdir('processed') if d.endswith('.csv')])\n",
    "os.makedirs('processed_clean', exist_ok=True)\n",
    "\n",
    "\n",
    "def convert_byte_strings(arr):\n",
    "    def decode_if_bytes(s): return s.strip(\"b'\") if isinstance(\n",
    "        s, str) and s.startswith(\"b'\") else s\n",
    "    vectorized_func = np.vectorize(decode_if_bytes)\n",
    "    return vectorized_func(arr)\n",
    "\n",
    "\n",
    "for file in tqdm(processed_files):\n",
    "    df = pd.read_csv(join('processed', file))\n",
    "    df = df.apply(convert_byte_strings)\n",
    "    df.to_csv(join('processed_clean', file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually rename csvs then check that they match main csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = [d for d in os.listdir('processed') if d.endswith('.csv')]\n",
    "print(f'Processed {len(processed_files)} datasets')\n",
    "dset_names_processed = [d.replace('.csv', '') for d in processed_files]\n",
    "for dset_name in dset_names_processed:\n",
    "    assert dset_name in vals, dset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovw_filt = ovw[ovw.Dataset.str.lower().isin(dset_names_processed)].drop(\n",
    "    columns=['Unnamed: 0']).reset_index().drop(columns=['index'])\n",
    "ovw_filt['Dataset'] = ovw_filt['Dataset'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ovw_filt.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See if the new data can be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, feature_names, target_col_names = imodels.get_clean_dataset(\n",
    "    'water-quality_multitask', return_target_col_names=True)\n",
    "print('shapes', X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = imodels.util.data_util.DSET_MULTITASK_NAMES\n",
    "# names = ['corel16k001']\n",
    "for name in tqdm(names):\n",
    "    try:\n",
    "        X, y, feature_names, target_col_names = imodels.get_clean_dataset(\n",
    "            name + '_multitask', return_target_col_names=True, convertna=False)\n",
    "        print('unique labels in each target of np array y', [\n",
    "              len(set(y[:, i])) for i in range(y.shape[1])])\n",
    "    except:\n",
    "        print('failed', name)\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
