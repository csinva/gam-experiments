{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:00<00:00, 245.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment varied these params: ['dataset_name', 'use_multitask', 'n_boosting_rounds', 'max_rounds']\n",
      "num_datasets run in different groups\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "use_multitask  n_boosting_rounds  max_rounds\n",
       "0              0                  5000          54\n",
       "1              8                  50            12\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sys\n",
    "import imodels\n",
    "import pmlb\n",
    "import imodelsx.process_results\n",
    "from collections import defaultdict\n",
    "sys.path.append('../experiments/')\n",
    "results_dir = '../results/multitask_gam_mar12/'\n",
    "\n",
    "r = imodelsx.process_results.get_results_df(results_dir)\n",
    "experiment_filename = '../experiments/03_multitask_gam.py'\n",
    "d = imodelsx.process_results.fill_missing_args_with_default(r, experiment_filename)\n",
    "d = imodelsx.process_results.average_over_seeds(\n",
    "    d, experiment_filename, key_to_average_over='seed'\n",
    ")\n",
    "d = d[~d.dataset_name.str.contains('_fri_')]\n",
    "\n",
    "\n",
    "# checking\n",
    "cols_varied = imodelsx.process_results.get_experiment_keys(\n",
    "    d, experiment_filename)\n",
    "print('experiment varied these params:', cols_varied)\n",
    "if not 'roc_auc_test' in d:\n",
    "    d['roc_auc_test'] = np.nan\n",
    "BEST_PARAMS = {\n",
    "    'linear_penalty': 'ridge',\n",
    "    'n_boosting_rounds': 0,\n",
    "    'interactions': 0.95,\n",
    "    'use_onehot_prior': 0,\n",
    "    'use_input_normalization': 1,\n",
    "    'use_internal_classifiers': 0,\n",
    "    'use_normalize_feature_targets': 0,\n",
    "    'train_frac': 0.8,\n",
    "    'use_fit_target_curves': 1,\n",
    "    'max_rounds': 5000,\n",
    "}\n",
    "for col in cols_varied:\n",
    "    assert col in ['dataset_name', 'use_multitask'] or col in BEST_PARAMS, col + 'must be in BEST_PARAMS!'\n",
    "\n",
    "print('num_datasets run in different groups',)\n",
    "display(d.groupby([x for x in cols_varied if not x == 'dataset_name']).size())\n",
    "\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(r[r.use_normalize_feature_targets], actually_delete=True)\n",
    "# r.to_pickle('../results/agg.pkl')\n",
    "# imodelsx.process_results.delete_runs_in_dataframe(r[(r.use_multitask == 0) * (r.linear_penalty != 'ridge')], actually_delete=True)\n",
    "# imodelsx.process_results.get_experiment_keys(r, experiment_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare baseline to best setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'standard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/imodelsx/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'standard'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m clas \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(regr) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(clas) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     88\u001b[0m     evaluate(clas, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc_test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 24\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(d, metric)\u001b[0m\n\u001b[1;32m     19\u001b[0m metric_table \u001b[38;5;241m=\u001b[39m metric_table\u001b[38;5;241m.\u001b[39mrename(\n\u001b[1;32m     20\u001b[0m     columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultitask\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# table of breakdowns\u001b[39;00m\n\u001b[1;32m     23\u001b[0m breakdown_idxs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmetric_table\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstandard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>=100 samples\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>=1000 samples\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>=10000 samples\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_samples\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>=10 features\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>=25 features\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWell predicted\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMulti-output\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_multitask\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPMLB\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(pmlb\u001b[38;5;241m.\u001b[39mdataset_names),\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFri\u001b[39m\u001b[38;5;124m'\u001b[39m: metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_fri_\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     34\u001b[0m }\n\u001b[1;32m     36\u001b[0m metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimprovement\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultitask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m     37\u001b[0m     metric_table[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     38\u001b[0m summ \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/imodelsx/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/imodelsx/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'standard'"
     ]
    }
   ],
   "source": [
    "# filter datasets for which there is a row with use_multitask=0 and use_multitask=1 (done running)\n",
    "def evaluate(d, metric='r2_test'):\n",
    "    d2 = d.groupby(['dataset_name'])\n",
    "    d2 = d2.filter(lambda x: len(x) == 2)\n",
    "\n",
    "    # pivot table for d2 based on dataset_name, use_multitask, and r2_test\n",
    "    metric_table = d2.pivot_table(index=['dataset_name'], columns=[\n",
    "        'use_multitask'], values=metric)\n",
    "\n",
    "    # add num_features based on dataset_name\n",
    "    n_features = d2.groupby(['dataset_name']).first().n_features\n",
    "    n_samples = d2.groupby(['dataset_name']).first().n_samples\n",
    "\n",
    "    # add some metadata filters\n",
    "    metric_table['num_features'] = n_features\n",
    "    metric_table['num_samples'] = n_samples\n",
    "    metric_table['feature_over_samples'] = metric_table['num_features'].astype(\n",
    "        int) / metric_table['num_samples'].astype(int)\n",
    "    metric_table = metric_table.rename(\n",
    "        columns={0: 'standard', 1: 'multitask'}).reset_index()\n",
    "\n",
    "    # table of breakdowns\n",
    "    breakdown_idxs = {\n",
    "        'All': metric_table['standard'] > -100,\n",
    "        '>=100 samples': metric_table['num_samples'] >= 100,\n",
    "        '>=1000 samples': metric_table['num_samples'] >= 1000,\n",
    "        '>=10000 samples': metric_table['num_samples'] >= 10000,\n",
    "        '>=10 features': metric_table['num_features'] >= 10,\n",
    "        '>=25 features': metric_table['num_features'] >= 25,\n",
    "        'Well predicted': metric_table['standard'] > 0.5,\n",
    "        'Multi-output': metric_table['dataset_name'].str.endswith('_multitask'),\n",
    "        'PMLB': metric_table['dataset_name'].isin(pmlb.dataset_names),\n",
    "        'Fri': metric_table['dataset_name'].str.contains('_fri_'),\n",
    "    }\n",
    "\n",
    "    metric_table['improvement'] = metric_table['multitask'] - \\\n",
    "        metric_table['standard']\n",
    "    summ = defaultdict(list)\n",
    "    for setting, idxs in breakdown_idxs.items():\n",
    "        if idxs.sum() > 0:\n",
    "            summ['setting'].append(setting)\n",
    "            summ['n_datasets'].append(idxs.sum())\n",
    "            summ['frac_improved'].append(\n",
    "                (metric_table['improvement'][idxs] > 0).mean())\n",
    "            summ['median_change'].append(\n",
    "                metric_table['improvement'][idxs].median())\n",
    "            summ['median_multitask'].append(\n",
    "                metric_table['multitask'][idxs].median())\n",
    "            summ['avg_change'].append(\n",
    "                metric_table['improvement'][idxs].mean())\n",
    "            summ['avg_multitask'].append(\n",
    "                metric_table['multitask'][idxs].mean())\n",
    "            # summ['avg_standard'].append(metric_table['standard'][idxs].mean())\n",
    "\n",
    "    # multioutput\n",
    "    display(pd.DataFrame(summ).round(3))\n",
    "    print('top dsets')\n",
    "    display(metric_table.sort_values(\n",
    "        'improvement', ascending=False).head(30).round(2))\n",
    "    # with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    # display(r2_table.sort_values('improvement', ascending=False).round(2))\n",
    "\n",
    "    # make plot\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "    # hue on log scale\n",
    "    metric_table['num_features_log'] = np.log(metric_table['num_features'])\n",
    "    sns.scatterplot(data=metric_table, x='standard',\n",
    "                    y='multitask', hue='num_features_log', alpha=0.5)\n",
    "    plt.legend(loc='center right', bbox_to_anchor=(\n",
    "        1.3, 0.35), title='log(num_features)')\n",
    "    plt.title(metric, fontsize='medium')\n",
    "    # plt.xlim([0.5, 1.05])\n",
    "    # plt.ylim([0.5, 1.05])\n",
    "    plt.savefig(f'../figs/{metric}.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# df = d\n",
    "df = d[(d[list(BEST_PARAMS)] == pd.Series(BEST_PARAMS)).all(axis=1)]\n",
    "\n",
    "# df = df[df.train_frac == 0.25]\n",
    "df = df[~df.dataset_name.str.contains('_fri_')]\n",
    "regr = df[df['r2_test'].notna()]\n",
    "clas = df[df['roc_auc_test'].notna()]\n",
    "if len(regr) > 0:\n",
    "    evaluate(regr, metric='r2_test')\n",
    "if len(clas) > 0:\n",
    "    evaluate(clas, metric='roc_auc_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num datasets run in each setting:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "use_multitask  n_boosting_rounds  max_rounds\n",
       "0              0                  5000          45\n",
       "1              8                  50            12\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 completed shared datasets\n"
     ]
    }
   ],
   "source": [
    "cols_varied = imodelsx.process_results.get_experiment_keys(\n",
    "    d, experiment_filename)\n",
    "# d = d[(d.use_internal_classifiers == 0) * (d.use_onehot_prior == 0)]\n",
    "cols_varied_d_ = [x for x in cols_varied if not x == 'dataset_name']\n",
    "groups = d.groupby(cols_varied_d_)\n",
    "\n",
    "dset_names = [set(d.loc[g]['dataset_name'].values)\n",
    "              for g in groups.groups.values()]\n",
    "dset_names_shared = list(set.intersection(*dset_names))\n",
    "print('Num datasets run in each setting:')\n",
    "display(groups.size())\n",
    "print(len(dset_names_shared), 'completed shared datasets')\n",
    "dc = d[d.dataset_name.isin(dset_names_shared)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chansingh/imodelsx/.venv/lib/python3.11/site-packages/pandas/io/formats/style.py:3807: RuntimeWarning: All-NaN slice encountered\n",
      "  smin = np.nanmin(gmap) if vmin is None else vmin\n",
      "/home/chansingh/imodelsx/.venv/lib/python3.11/site-packages/pandas/io/formats/style.py:3808: RuntimeWarning: All-NaN slice encountered\n",
      "  smax = np.nanmax(gmap) if vmax is None else vmax\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fe85f_row0_col3, #T_fe85f_row0_col6, #T_fe85f_row1_col7 {\n",
       "  background-color: #fde725;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_fe85f_row0_col4, #T_fe85f_row0_col7, #T_fe85f_row1_col3, #T_fe85f_row1_col4, #T_fe85f_row1_col6 {\n",
       "  background-color: #440154;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_fe85f_row0_col5, #T_fe85f_row1_col5 {\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fe85f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fe85f_level0_col0\" class=\"col_heading level0 col0\" >use_multitask</th>\n",
       "      <th id=\"T_fe85f_level0_col1\" class=\"col_heading level0 col1\" >n_boosting_rounds</th>\n",
       "      <th id=\"T_fe85f_level0_col2\" class=\"col_heading level0 col2\" >max_rounds</th>\n",
       "      <th id=\"T_fe85f_level0_col3\" class=\"col_heading level0 col3\" >r2_test</th>\n",
       "      <th id=\"T_fe85f_level0_col4\" class=\"col_heading level0 col4\" >win_rate</th>\n",
       "      <th id=\"T_fe85f_level0_col5\" class=\"col_heading level0 col5\" >r2_test__>=25_features</th>\n",
       "      <th id=\"T_fe85f_level0_col6\" class=\"col_heading level0 col6\" >r2_test_median</th>\n",
       "      <th id=\"T_fe85f_level0_col7\" class=\"col_heading level0 col7\" >r2_train</th>\n",
       "      <th id=\"T_fe85f_level0_col8\" class=\"col_heading level0 col8\" >r2_train_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fe85f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fe85f_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_fe85f_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_fe85f_row0_col2\" class=\"data row0 col2\" >5000</td>\n",
       "      <td id=\"T_fe85f_row0_col3\" class=\"data row0 col3\" >0.612</td>\n",
       "      <td id=\"T_fe85f_row0_col4\" class=\"data row0 col4\" >nan</td>\n",
       "      <td id=\"T_fe85f_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "      <td id=\"T_fe85f_row0_col6\" class=\"data row0 col6\" >0.681</td>\n",
       "      <td id=\"T_fe85f_row0_col7\" class=\"data row0 col7\" >0.765</td>\n",
       "      <td id=\"T_fe85f_row0_col8\" class=\"data row0 col8\" >0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fe85f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fe85f_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_fe85f_row1_col1\" class=\"data row1 col1\" >8</td>\n",
       "      <td id=\"T_fe85f_row1_col2\" class=\"data row1 col2\" >50</td>\n",
       "      <td id=\"T_fe85f_row1_col3\" class=\"data row1 col3\" >0.550</td>\n",
       "      <td id=\"T_fe85f_row1_col4\" class=\"data row1 col4\" >0.167</td>\n",
       "      <td id=\"T_fe85f_row1_col5\" class=\"data row1 col5\" >nan</td>\n",
       "      <td id=\"T_fe85f_row1_col6\" class=\"data row1 col6\" >0.655</td>\n",
       "      <td id=\"T_fe85f_row1_col7\" class=\"data row1 col7\" >0.778</td>\n",
       "      <td id=\"T_fe85f_row1_col8\" class=\"data row1 col8\" >0.905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9cb1b9aa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute stats per group\n",
    "baseline_group_idx = 0\n",
    "groups = dc.groupby(cols_varied_d_)\n",
    "group_idxs = list(groups.groups.values())\n",
    "baseline_group = dc.loc[group_idxs[baseline_group_idx].values].sort_values(\n",
    "    by='dataset_name')\n",
    "stat_cols = defaultdict(list)\n",
    "for group in groups.groups.values():\n",
    "    g = dc.loc[group].sort_values(by='dataset_name')\n",
    "    stat_cols['win_rate'].append(\n",
    "        (g['r2_test'].values >\n",
    "         baseline_group['r2_test'].values).mean())\n",
    "    stat_cols['r2_test__>=25_features'] = g[g['n_features']\n",
    "                                            >= 25]['r2_test'].mean()\n",
    "    for k in ['r2_test', 'r2_train']:\n",
    "        stat_cols[k].append(g[k].mean())\n",
    "        stat_cols[f'{k}_median'].append(g[k].median())\n",
    "stat_cols = pd.DataFrame(stat_cols)\n",
    "\n",
    "# save with index\n",
    "stats = groups['r2_test'].mean().reset_index()\n",
    "for col in stat_cols.columns:\n",
    "    stats[col] = stat_cols[col].values\n",
    "stats.loc[baseline_group_idx, 'win_rate'] = np.nan\n",
    "\n",
    "\n",
    "# color last 2 columns by value\n",
    "display(\n",
    "    stats\n",
    "    .style\n",
    "    .background_gradient(\n",
    "        cmap='viridis', subset=['r2_test', 'r2_test_median', 'win_rate', 'r2_test__>=25_features', 'r2_train']\n",
    "    )\n",
    "    .format(precision=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train frac plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 3))\n",
    "for i, met in enumerate(['r2_test', 'r2_test_median', 'win_rate']):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    tab = stats.pivot_table(index=['train_frac'], columns=[\n",
    "        'use_multitask'], values=met)\n",
    "    plt.plot(tab, 'o-', label=[{0: 'Single-task',\n",
    "             1: 'Multi-task'}[x] for x in tab.columns])\n",
    "    plt.ylabel(met)\n",
    "    plt.xlabel('Fraction of data used for training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".embgam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "559535f78d940c882783b39501b2581b5193373045707e5f8a51d046029cfd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
