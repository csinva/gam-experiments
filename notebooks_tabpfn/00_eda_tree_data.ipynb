{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sys\n",
    "import imodels\n",
    "import imodelsx.process_results\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "# data\n",
    "n = 2000\n",
    "d = 20\n",
    "test_frac = 0.2\n",
    "\n",
    "# DGP\n",
    "n_classes = 2\n",
    "n_informative = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing tree depth experiment\n",
    "\n",
    "# generate synthetic data from decision trees of increasing depths and test tabpfn acc\n",
    "r = defaultdict(list)\n",
    "depths = [1, 2, 3, 4, 5]  # , 6, 7, 8, 9, 10]\n",
    "r['depth'] = depths\n",
    "for depth in tqdm(depths):\n",
    "    X, y = make_classification(\n",
    "        n_samples=n, n_features=d, n_informative=n_informative, n_classes=n_classes, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_frac, random_state=42)\n",
    "    tree = DecisionTreeClassifier(max_depth=depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_train_tree = tree.predict(X_train)\n",
    "    y_test_tree = tree.predict(X_test)\n",
    "\n",
    "    classifier = TabPFNClassifier(device='cuda')\n",
    "    classifier.fit(X_train, y_train_tree)\n",
    "    y_test_tabpfn = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test_tree, y_test_tabpfn)\n",
    "    r['acc'].append(acc)\n",
    "    print(acc)\n",
    "\n",
    "r = pd.DataFrame(r)\n",
    "sns.lineplot(data=r, x='depth', y='acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "X, y = make_regression(\n",
    "    n_samples=n, n_features=d, n_informative=n_informative, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_frac, random_state=42)\n",
    "regressor = TabPFNRegressor(device='cuda', n_estimators=1)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_test_tabpfn = regressor.predict(X_test)\n",
    "print('r2 test', r2_score(y_test, y_test_tabpfn))\n",
    "\n",
    "# classification\n",
    "X, y = make_classification(\n",
    "    n_samples=n, n_features=d, n_informative=n_informative, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_frac, random_state=42)\n",
    "classifier = TabPFNClassifier(device='cuda', n_estimators=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_tabpfn = classifier.predict(X_test)\n",
    "print('acc test', accuracy_score(y_test, y_test_tabpfn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init shape (400, 20)\n",
      "input preprocessed shape (400, 20)\n",
      "init output torch.Size([400, 10])\n",
      "filtered outputs, one for each classifier [torch.Size([400, 2])]\n",
      "enc sizes are batch_size, num_samples (train + test), n_heads, d_output?\n",
      "dec sizes are (num_samples [test], 1, 10 classes [actual values are the first n_classes, rest are dropped])\n",
      "enc_attn_0: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_0: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_0: torch.Size([1, 12, 1600, 192])\n",
      "enc_0: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_1: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_1: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_1: torch.Size([1, 12, 1600, 192])\n",
      "enc_1: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_2: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_2: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_2: torch.Size([1, 12, 1600, 192])\n",
      "enc_2: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_3: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_3: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_3: torch.Size([1, 12, 1600, 192])\n",
      "enc_3: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_4: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_4: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_4: torch.Size([1, 12, 1600, 192])\n",
      "enc_4: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_5: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_5: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_5: torch.Size([1, 12, 1600, 192])\n",
      "enc_5: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_6: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_6: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_6: torch.Size([1, 12, 1600, 192])\n",
      "enc_6: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_7: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_7: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_7: torch.Size([1, 12, 1600, 192])\n",
      "enc_7: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_8: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_8: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_8: torch.Size([1, 12, 1600, 192])\n",
      "enc_8: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_9: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_9: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_9: torch.Size([1, 12, 1600, 192])\n",
      "enc_9: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_10: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_10: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_10: torch.Size([1, 12, 1600, 192])\n",
      "enc_10: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_11: torch.Size([1, 12, 1600, 192])\n",
      "enc_attn_f_11: torch.Size([1, 2000, 12, 192])\n",
      "enc_attn_i_11: torch.Size([1, 12, 1600, 192])\n",
      "enc_11: torch.Size([1, 2000, 12, 192])\n",
      "dec_0: torch.Size([400, 1, 768])\n",
      "dec_1: torch.Size([400, 1, 768])\n",
      "dec_2: torch.Size([400, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# model = regressor\n",
    "model = classifier\n",
    "m = model.model_\n",
    "activations = {}\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "# Register hooks on specific layers (example: Transformer blocks)\n",
    "for idx, block in enumerate(m.transformer_encoder.layers):\n",
    "    block.self_attn_between_features.register_forward_hook(\n",
    "        get_activation(f'enc_attn_f_{idx}'))\n",
    "for idx, block in enumerate(m.transformer_encoder.layers):\n",
    "    block.self_attn_between_items.register_forward_hook(\n",
    "        get_activation(f'enc_attn_i_{idx}'))\n",
    "for idx, block in enumerate(m.transformer_encoder.layers):\n",
    "    block.register_forward_hook(get_activation(f'enc_{idx}'))\n",
    "for idx, block in enumerate(m.decoder_dict.standard):\n",
    "    block.register_forward_hook(get_activation(f'dec_{idx}'))\n",
    "\n",
    "# Fit model (for inference)\n",
    "preds = model.predict_proba(X_test)\n",
    "# preds = model.predict(X_test, output_type='mode')\n",
    "\n",
    "# Display extracted activations\n",
    "\n",
    "print('enc sizes are batch_size, num_samples (train + test), n_heads, d_output?')\n",
    "# note: half the heads are training heads, whereas the other half are inference heads which do not attend to each other\n",
    "print(\n",
    "    'dec sizes are (num_samples [test], 1, 10 classes [actual values are the first n_classes, rest are dropped])')\n",
    "# note: regression basically does 5000-class classification over quantiles then averages them\n",
    "# (classification is easier to study)\n",
    "for layer, activation in activations.items():\n",
    "    print(f\"{layer}: {activation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training False\n",
      "_input_size 192\n",
      "_output_size 192\n",
      "_d_k 32\n",
      "_d_v 32\n",
      "_nhead 6\n",
      "_nhead_kv 6\n",
      "recompute False\n",
      "init_gain 1.0\n",
      "two_sets_of_queries False\n"
     ]
    }
   ],
   "source": [
    "d = vars(m.transformer_encoder.layers[0].self_attn_between_features)\n",
    "# only show elements with scalar values\n",
    "for k, v in d.items():\n",
    "    if np.isscalar(v):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerFeatureTransformer(\n",
       "  (encoder): SequentialEncoder(\n",
       "    (0): RemoveEmptyFeaturesEncoderStep()\n",
       "    (1): NanHandlingEncoderStep()\n",
       "    (2): VariableNumFeaturesEncoderStep()\n",
       "    (3): InputNormalizationEncoderStep()\n",
       "    (4): VariableNumFeaturesEncoderStep()\n",
       "    (5): LinearInputEncoderStep(\n",
       "      (layer): Linear(in_features=4, out_features=192, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (y_encoder): SequentialEncoder(\n",
       "    (0): NanHandlingEncoderStep()\n",
       "    (1): MulticlassClassificationTargetEncoder()\n",
       "    (2): LinearInputEncoderStep(\n",
       "      (layer): Linear(in_features=2, out_features=192, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (transformer_encoder): LayerStack(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x PerFeatureEncoderLayer(\n",
       "        (self_attn_between_features): MultiHeadAttention()\n",
       "        (self_attn_between_items): MultiHeadAttention()\n",
       "        (mlp): MLP(\n",
       "          (linear1): Linear(in_features=192, out_features=768, bias=False)\n",
       "          (linear2): Linear(in_features=768, out_features=192, bias=False)\n",
       "        )\n",
       "        (layer_norms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((192,), eps=1e-05, elementwise_affine=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_dict): ModuleDict(\n",
       "    (standard): Sequential(\n",
       "      (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=768, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_positional_embedding_embeddings): Linear(in_features=48, out_features=192, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
